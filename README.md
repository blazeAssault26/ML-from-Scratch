# ML-from-Scratch

This repository explores fundamental machine learning algorithms implemented from scratch, delving into the underlying mathematics. It uses visualizations to illustrate the internals of each algorithm, enhancing comprehension of their principles and mechanisms.

## Overview
Boosting is an ensemble learning technique aimed at improving model performance by iteratively combining weak learners, such as decision trees. Unlike traditional ensemble methods like bagging, boosting corrects errors made by previous models sequentially. This adaptive approach enables boosting to capture complex data relationships and achieve high predictive accuracy. This notebook demonstrates Gradient Boosting Machines (GBM), which improve predictions by fitting decision trees to the residuals of previous models. This iterative process refines predictions and minimizes errors. By adapting to data nuances through residual analysis, GBM enhances predictive power beyond individual weak learners. GBM employs additive modeling, where each new tree is added to the model to correct the errors of the previous trees, iteratively reducing the residuals and minimizing the error to zero over subsequent models. 
