# ML-from-Scratch

This repository explores fundamental machine learning algorithms implemented from scratch, delving into the underlying mathematics. It uses visualizations to illustrate the internals of each algorithm, enhancing comprehension of their principles and mechanisms.

## Overview

**_Boosting_** is an ensemble learning technique aimed at improving model performance by iteratively combining weak learners, such as decision trees. Unlike traditional ensemble methods like bagging, boosting corrects errors made by previous models sequentially. This adaptive approach enables boosting to capture complex data relationships and achieve high predictive accuracy. This notebook demonstrates Gradient Boosting Machines (GBM), which improve predictions by fitting decision trees to the residuals of previous models. This iterative process refines predictions and minimizes errors. By adapting to data nuances through residual analysis, GBM enhances predictive power beyond individual weak learners. GBM employs additive modeling, where each new tree is added to the model to correct the errors of the previous trees, iteratively reducing the residuals and minimizing the error to zero over subsequent models.

**_Linear regression_** is a foundational statistical technique used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The objective is to find the best-fitting line that minimizes the sum of squared residuals between observed and predicted values. Mathematically, it involves estimating the coefficients ((Î²)) in the equation (ğ‘¦ = ğ›½â‚€ + ğ›½â‚ğ‘¥â‚ + â‹¯ + ğ›½â‚™ğ‘¥â‚™ + Ïµ) using the Ordinary Least Squares (OLS) method. This method minimizes the sum of the squared differences between observed and predicted values, achieved by solving the normal equation (ğ›½ = (ğ‘‹áµ€ğ‘‹)â»Â¹ğ‘‹áµ€ğ‘¦) where 
X is the matrix of input features and ğ‘¦ is the vector of output values.This project demonstrates linear regression by implementing it from scratch and comparing it with the scikit-learn library. The custom implementation calculates the slope and intercept manually showcasing the underlying maths, while scikit-learn uses the efficient OLS method internally.

**_K-Nearest Neighbors (KNN)_** is a simple, non-parametric, lazy learning algorithm used for classification and regression. The core idea is to predict the class of a given sample based on the majority class among its k-nearest neighbors in the training set.In this notebook, a custom (KNN) classifier is implemented to classify the Iris dataset. The distance between data points is typically measured using Euclidean distance, given by the formula: `distance(xâ‚, xâ‚‚) = âˆš(âˆ‘(i=1 to n) (xâ‚áµ¢ - xâ‚‚áµ¢)Â²)`. One significant characteristic of KNN is that it is a lazy learner, meaning it does not learn a discriminative function from the training data but rather memorizes the training dataset, leading to high computational costs during prediction, especially with large datasets, as the model performs the task of finding nearest neighbors and determining the majority class only when the `predict` function is called. The model only "learns" the parameters during the `fit` process by storing the training data. In the provided implementation, the KNN classifier is built using a custom class `my_KNN`, which includes methods to compute distances, find the nearest neighbors, and predict the class for new samples. The classifier is trained and evaluated on the Iris dataset, demonstrating its usage and the calculation of classification accuracy. Additionally, KNN is sensitive to the curse of dimensionality, where the distance metric becomes less informative as the number of dimensions increases, and to outliers, which can significantly skew the results.
