{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Coding a Layer of 3 neurons , each being fed 4 ip and having a bias each\n",
        "\n",
        "ip = [1,2,3,2.5]\n",
        "\n",
        "weights = [[0.2,0.8,-0.5,1],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]] #list of list\n",
        "\n",
        "# Importance of biases, why sometimes initialise to non zero value -> suppose the neuron does not fire and bias=0 , when op of one layer-> ip of next will be 0\n",
        "# hence that neuron will also not fire and if the bias for that also=0 the neural nw is dead\n",
        "\n",
        "bias = [2,3,0.5]\n",
        "layer_op=[]\n",
        "\n",
        "for neuron_wt, neuron_bias in zip(weights,bias):\n",
        "  neuron_op = 0\n",
        "  for n_ip,weight in zip(ip,neuron_wt):\n",
        "    neuron_op +=n_ip*weight\n",
        "  neuron_op += neuron_bias\n",
        "  layer_op.append(neuron_op)\n",
        "\n",
        "layer_op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuTfix-I5GLg",
        "outputId": "5efdac8b-4358-42f4-f993-b90c9fdaae17"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.8, 1.21, 2.385]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just use dot product from numpy to ease the codework->\n",
        "import numpy as np\n",
        "\n",
        "layer_op = np.dot(weights,ip) +bias # typical matrix and vector multiplication so when\n",
        "# (matrix,vector) -> vector(4,1) and for (vector,matrix) ->vector(1,4),hence error in this case\n",
        "\n",
        "layer_op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3JUwjmw5GQN",
        "outputId": "aa2fd892-284d-4cac-b783-f2bebd43cf59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.8  , 1.21 , 2.385])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# why batches??\n",
        "# more parallel computation and better generalizations\n",
        "#increasing the batch size to optimum size ,generally power of 2; leads to better fitting/training of the neuron\n",
        "# if all shown at same time ->' OVERFITTING'"
      ],
      "metadata": {
        "id": "ssY-w3T_5GSY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we have 3 sets of ip\n",
        "\n",
        "ip = [[1,2,3,2.5],\n",
        "      [2,5,1,2],\n",
        "      [-1.5,2.7,3.3,-.8]]\n",
        "\n",
        "# 3 neurons with diff weights for diff ip\n",
        "\n",
        "weights = [[0.2,0.8,-0.5,1],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "bias = [2,3,0.5]\n",
        "layer_op=[]"
      ],
      "metadata": {
        "id": "Q_o8q2x75GUa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "op  =np.dot(ip,np.array(weights).T) +bias\n",
        "op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2wYoiPa5GWu",
        "outputId": "83fd7855-c033-4347-b471-ad492a4c0ca5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.8  ,  1.21 ,  2.385],\n",
              "       [ 7.9  , -1.29 ,  0.54 ],\n",
              "       [ 1.41 ,  1.051,  0.026]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding a layer to the network\n",
        "weights2 = [[0.1,-0.14,0.5],\n",
        "           [-0.5, 0.12, -0.33],\n",
        "           [-0.44, 0.73, -0.13]]\n",
        "\n",
        "bias2 = [-1,.2,-0.5]\n",
        "op2= np.dot(op, weights2)+bias2\n",
        "op2\n",
        "\n"
      ],
      "metadata": {
        "id": "IiCc57Ph5GYy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6003b1f9-a914-495e-b5b5-679b6f0eb815"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.1744 ,  1.41425,  1.19065],\n",
              "       [ 0.1974 , -0.6666 ,  3.8055 ],\n",
              "       [-1.39594,  0.1477 , -0.14521]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialising a layer\n",
        "# either we have stored weights and bias from our pvs saved models or we build on our own\n",
        "# we need weights ,typically bw -1 and 1 to avoid the problem of exploding gradients later since we will be having a weighted sum of ip"
      ],
      "metadata": {
        "id": "3bKGJxRo5Gaz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "X = [[1, 2, 3, 2.5],                 #passing 3 batch of ip with 4 features\n",
        "     [2.0, 5.0, -1.0, 2.0],\n",
        "     [-1.5, 2.7, 3.3, -0.8]]"
      ],
      "metadata": {
        "id": "TJhE5BMl5GdU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.1*np.random.randn(n_inputs,n_neurons)  # here the shape is set to ip,neurons to avoid transpose op in the forward pass\n",
        "    self.bias = np.zeros((1,n_neurons))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    self.op = np.dot(inputs,self.weights) +self.bias\n"
      ],
      "metadata": {
        "id": "MFaJBZ-k5Gf2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_1 =Layer_Dense(4,5)  #here the ip should be same as ip columns\n",
        "layer_2 =Layer_Dense(5,2) #here the ip shape should match the op of previous layer"
      ],
      "metadata": {
        "id": "L-OI1Rpo5GiM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_1.forward(X)\n",
        "layer_1.op"
      ],
      "metadata": {
        "id": "CVmHWNaA5Gks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8acf8837-6bb2-43cc-b930-e1ff3e03388e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.10758131,  1.03983522,  0.24462411,  0.31821498,  0.18851053],\n",
              "       [-0.08349796,  0.70846411,  0.00293357,  0.44701525,  0.36360538],\n",
              "       [-0.50763245,  0.55688422,  0.07987797, -0.34889573,  0.04553042]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_2.forward(layer_1.op)\n",
        "layer_2.op"
      ],
      "metadata": {
        "id": "DHB0T0Co5Gm_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260b0eb2-eba5-442b-b87e-8cdbb665dca9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.148296  , -0.08397602],\n",
              "       [ 0.14100315, -0.01340469],\n",
              "       [ 0.20124979, -0.07290616]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# why we need an activation function in first place?? 'NON-LINEARITY'\n",
        "\n",
        "# if we dont have non linear activation :\n",
        "#'PROBLEM'-> our model is basically a linear function/mapping of the ip, so we can fit a linear function or\n",
        "# at max approx a non linear function inefficiently"
      ],
      "metadata": {
        "id": "cDZb-i_wmDEy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use of sigmoid for step function -> missing the granularity in knowing how close was our model in op a 1 or a 0\n",
        "# use of relu for sigmoid-> it is faster and the problem of vanishing gradients with the sigmoid"
      ],
      "metadata": {
        "id": "X2j2ZcZOLFl_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHxocUbkmDHi",
        "outputId": "32900fea-8556-44ea-a987-70acac8ef91a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from nnfs) (1.25.2)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = spiral_data(100, 3)"
      ],
      "metadata": {
        "id": "q927qZh3QEVi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLe6jObG5WS6",
        "outputId": "aff28947-5da8-460a-cb6b-89f1c34db406"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation_relu:\n",
        "  def forward(self,input):\n",
        "    self.op = np.maximum(0,input)\n"
      ],
      "metadata": {
        "id": "F1VAIHcemDKP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = 0.1*np.random.randn(n_inputs,n_neurons)\n",
        "    self.bias = np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.op = np.dot(inputs,self.weights) +self.bias\n",
        "\n",
        "activation1 = Activation_relu()"
      ],
      "metadata": {
        "id": "RnsXaTY4mDMm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_1 =Layer_Dense(2,5)   #spiral dataset has 2 columns\n",
        "layer_1.forward(X)\n",
        "layer_1.op\n",
        "activation1.forward(layer_1.op)"
      ],
      "metadata": {
        "id": "HnINLB5DmDO-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_1.op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzs6sHAomDSF",
        "outputId": "6205c09c-49b4-49f5-f9d3-2d0451a0d108"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00],\n",
              "       [-8.35815910e-04, -7.90404272e-04, -1.33452227e-03,\n",
              "         4.65504505e-04,  4.56846210e-05],\n",
              "       [-2.39994470e-03,  5.93469958e-05, -2.24808278e-03,\n",
              "         2.03573116e-04,  6.10024377e-04],\n",
              "       ...,\n",
              "       [ 1.13291524e-01, -1.89262271e-01, -2.06855070e-02,\n",
              "         8.11079666e-02, -6.71350807e-02],\n",
              "       [ 1.34588361e-01, -1.43197834e-01,  3.09493970e-02,\n",
              "         5.66337556e-02, -6.29687458e-02],\n",
              "       [ 1.07817926e-01, -2.00809643e-01, -3.37579325e-02,\n",
              "         8.72561932e-02, -6.81458861e-02]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activation1.op #all negative values are clipped to 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdtjGdyDmDVc",
        "outputId": "12dbadac-b0b0-4d12-a27a-4de637ed98b1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "        0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.65504505e-04,\n",
              "        4.56846210e-05],\n",
              "       [0.00000000e+00, 5.93469958e-05, 0.00000000e+00, 2.03573116e-04,\n",
              "        6.10024377e-04],\n",
              "       ...,\n",
              "       [1.13291524e-01, 0.00000000e+00, 0.00000000e+00, 8.11079666e-02,\n",
              "        0.00000000e+00],\n",
              "       [1.34588361e-01, 0.00000000e+00, 3.09493970e-02, 5.66337556e-02,\n",
              "        0.00000000e+00],\n",
              "       [1.07817926e-01, 0.00000000e+00, 0.00000000e+00, 8.72561932e-02,\n",
              "        0.00000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use of softmax for relu ??\n",
        "#how wrong is our model when 2 accuracy is same  for both ??\n",
        "\n",
        "# relu gives unbounded op [0,infinity], learning from clipped values ->ineffetive learning\n",
        "# cannot use absolute function since lose the meaning of negative values entirely hence exponential\n",
        "# then normalisation to get the probabilty distribution required ->> \"SOFTMAX\""
      ],
      "metadata": {
        "id": "buuhS0zc5Gsy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class activation_softmax:\n",
        "  def forward(self, inputs):\n",
        "    exp_values = np.exp(inputs-np.max(inputs,axis=1,keepdims=True))\n",
        "    probability = exp_values /(np.sum(exp_values, axis=1, keepdims =True))\n",
        "    self.op = probability"
      ],
      "metadata": {
        "id": "bOaVAPUdjvyE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_2 = Layer_Dense(5, 3) # op of first layer is 5 and since we have 3 classes, so our op layer should have 3 neurons\n",
        "activation_2 = activation_softmax()\n",
        "\n",
        "layer_2.forward(activation1.op)\n",
        "activation_2.forward(layer_2.op)\n",
        "activation_2.op[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4Z_8jyKjv1O",
        "outputId": "af98412a-1d1d-4e64-90a5-5c973b640e60"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.33333334, 0.33333334, 0.33333334],\n",
              "       [0.33334148, 0.3333302 , 0.33332834],\n",
              "       [0.33335316, 0.33332598, 0.33332086],\n",
              "       [0.333332  , 0.33330762, 0.3333604 ],\n",
              "       [0.33333603, 0.33330083, 0.33336315]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss functions ??\n",
        "\n",
        "# our model gives an op in form of probability ditsribution instead of classes, so it would be more convinient for the optimizer to know that\n",
        "# what actually was the confidence with which our model was able to correclty classify or not\n",
        "# in classification we use categorically cross-entropy"
      ],
      "metadata": {
        "id": "lyQkc7VE1oNr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class loss:\n",
        "  def calculate(self,op,y):\n",
        "    sample_loss = self.forward(op,y)\n",
        "    data_loss = np.mean(sample_loss)\n",
        "    return data_loss\n",
        "\n",
        "class loss_categoricalcrossentropy(loss):\n",
        "  def forward(self,y_pred,y_true):\n",
        "    sample = len(y_pred)\n",
        "    y_pred_clipped =np.clip(y_pred,1e-7, 1-1e-7) # taking care of the instance when we get a mean(- log(0))\n",
        "\n",
        "# handling the cases where y_true is passed as a scaler or one hot encoded vectors\n",
        "\n",
        "    if len(y_true.shape) == 1:    # if scalar ->(batch_size,)\n",
        "      correct_confidences = y_pred_clipped[range(sample), y_true]\n",
        "    if len(y_true.shape) ==2:     #if one hot encoded vectors ->(batch_size,num_classes)\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n"
      ],
      "metadata": {
        "id": "tKfMk2B21oQh"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = loss_categoricalcrossentropy()\n",
        "loss = loss_function.calculate(activation_2.op, y)\n",
        "\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oK9oOQQ6bBZ",
        "outputId": "2f096333-8136-4cb6-af9c-7e0a28473243"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0988972"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}